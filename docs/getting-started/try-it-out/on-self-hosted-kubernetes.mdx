---
title: On Self-Hosted Kubernetes
description: Run OpenChoreo on any self-hosted Kubernetes cluster - local machine, VM, or on-premise. Zero cloud costs.
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import Link from '@docusaurus/Link';
import {versions} from '../../_constants.mdx';

# On Self-Hosted Kubernetes

Try OpenChoreo on any self-hosted Kubernetes cluster - whether it's running on your laptop, a VM, or on-premise environment. This is the fastest way to explore OpenChoreo without cloud provider costs.

**What you'll get:**
- Full OpenChoreo installation on your cluster
- All four planes: Control, Data, Build, and Observability
- Access via `.localhost` domains
- ~15-20 minutes to complete

## Prerequisites

<Tabs groupId="cluster-type">
<TabItem value="existing" label="Existing Cluster" default>

- **Kubernetes 1.32+** cluster with at least **8 GB RAM** and **4 CPU** cores
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+ configured to access your cluster
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+
- **cert-manager** installed in your cluster

```bash
kubectl version
helm version --short
kubectl get nodes
kubectl auth can-i '*' '*' --all-namespaces
```

**Note Your Ingress Configuration**

Check if you have an ingress controller:

```bash
kubectl get ingressclass
```

Common configurations:
- **Rancher Desktop**: Built-in Traefik on ports 80/443, class name `traefik`
- **Docker Desktop**: No default ingress
- **OrbStack**: Built-in ingress on ports 80/443

</TabItem>
<TabItem value="k3d" label="k3d">

[k3d](https://k3d.io) runs k3s in Docker containers.

- **Docker** v26.0+ with at least **8 GB RAM** and **4 CPU** cores allocated
- **Disk space**: ~10 GB free
- **[k3d](https://k3d.io/stable/#installation)** v5.8+
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+
- **cert-manager** installed in your cluster

```bash
docker --version && docker info > /dev/null
k3d --version
kubectl version --client
helm version --short
```

:::note Colima Users
Set `K3D_FIX_DNS=0` when creating clusters to avoid DNS issues. See [k3d-io/k3d#1449](https://github.com/k3d-io/k3d/issues/1449).
:::

**Create Cluster**

<CodeBlock language="bash">
{`curl -s https://raw.githubusercontent.com/openchoreo/openchoreo/${versions.githubRef}/install/k3d/single-cluster/config.yaml | k3d cluster create --config=-`}
</CodeBlock>

This creates a cluster named `openchoreo` with:
- 1 server node (no agents)
- Port mappings: Control Plane (8080/8443), Data Plane (19080/19443), Build Plane (10081)
- kubectl context set to `k3d-openchoreo`

</TabItem>
<TabItem value="kind" label="kind">

[kind](https://kind.sigs.k8s.io) runs Kubernetes in Docker containers.

- **Docker** v26.0+ with at least **8 GB RAM** and **4 CPU** cores allocated
- **Disk space**: ~10 GB free
- **[kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation)** v0.30+
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+
- **cert-manager** installed in your cluster


```bash
docker --version && docker info > /dev/null
kind --version
kubectl version --client
helm version --short
```

**Create Cluster**

```bash
cat <<EOF | kind create cluster --name openchoreo --config=-
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: kindest/node:v1.32.0
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 8080
    protocol: TCP
  - containerPort: 443
    hostPort: 8443
    protocol: TCP
- role: worker
  image: kindest/node:v1.32.0
EOF
```

**Install cloud-provider-kind**

KIND doesn't have a built-in load balancer. Install [cloud-provider-kind](https://github.com/kubernetes-sigs/cloud-provider-kind) to enable LoadBalancer services for the data plane gateway:

```bash
docker run -d --rm --name cloud-provider-kind --network kind \
  -v /var/run/docker.sock:/var/run/docker.sock \
  ghcr.io/kubernetes-sigs/cloud-provider-kind
```

</TabItem>
<TabItem value="minikube" label="minikube">

[minikube](https://minikube.sigs.k8s.io) creates a local Kubernetes cluster in a VM or container.

- **Docker** v26.0+ OR a hypervisor (VirtualBox, Hyper-V, etc.)
- **8 GB RAM** and **4 CPU** cores available
- **Disk space**: ~10 GB free
- **[minikube](https://minikube.sigs.k8s.io/docs/start/)** v1.32+
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+
- **cert-manager** installed in your cluster

```bash
minikube version
kubectl version --client
helm version --short
```

**Create Cluster**

```bash
minikube start --cpus=4 --memory=8192 --driver=docker --profile=openchoreo
minikube addons enable ingress --profile=openchoreo
```

:::note Port Access
minikube requires `minikube tunnel` to access LoadBalancer services. Run it in a separate terminal:
```bash
minikube tunnel --profile=openchoreo
```
:::

</TabItem>
<TabItem value="k3s" label="k3s">

- **Linux** system with at least **8 GB RAM** and **4 CPU** cores
- **Root access** or sudo privileges
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+
- **cert-manager** installed in your cluster

**Configure Registry Mirror (Required for Build Plane)**

Before installing k3s, configure containerd to access the internal build plane registry. This is required because the kubelet runs on the host and cannot resolve Kubernetes internal DNS names.

```bash
sudo mkdir -p /etc/rancher/k3s
sudo tee /etc/rancher/k3s/registries.yaml > /dev/null << 'EOF'
mirrors:
  "registry.openchoreo-build-plane.svc.cluster.local:5000":
    endpoint:
      - "http://registry.openchoreo-build-plane.svc.cluster.local:5000"
configs:
  "registry.openchoreo-build-plane.svc.cluster.local:5000":
    tls:
      insecure_skip_verify: true
EOF
```

**Install k3s**

```bash
curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644
```

Configure kubectl:

```bash
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
# Or copy to default location:
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $(id -u):$(id -g) ~/.kube/config
```

</TabItem>
</Tabs>

<details>
<summary>Install cert-manager</summary>

```bash
helm upgrade --install cert-manager oci://quay.io/jetstack/charts/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --set crds.enabled=true
```

Wait for cert-manager to be ready:

```bash
kubectl wait --for=condition=available deployment/cert-manager -n cert-manager --timeout=120s
```

</details>

---

## Step 1: Setup Control Plane

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-control-plane ${versions.helmSource}/openchoreo-control-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-control-plane \\
    --create-namespace \\
    --set global.baseDomain=openchoreo.localhost \\
    --set global.port=":8080" \\
    --set traefik.ports.web.exposedPort=8080 \\
    --set traefik.ports.websecure.exposedPort=8443 \\
    --set thunder.configuration.server.publicUrl=http://thunder.openchoreo.localhost:8080 \\
    --set thunder.configuration.gateClient.hostname=thunder.openchoreo.localhost \\
    --set thunder.configuration.gateClient.port=8080 \\
    --set thunder.configuration.gateClient.scheme="http"`}
</CodeBlock>

Wait for pods to be ready:

```bash
kubectl get pods -n openchoreo-control-plane -w
```

---

## Step 2: Setup Data Plane

:::note macOS Users
If you're on macOS with Docker Desktop, k3d, kind, or Colima (with Rosetta), add `--set gateway.envoy.mountTmpVolume=true` to fix Envoy temporary file issues.
:::

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-data-plane ${versions.helmSource}/openchoreo-data-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-data-plane \\
    --create-namespace \\
    --set gateway.httpPort=19080 \\
    --set gateway.httpsPort=19443 \\
    --set external-secrets.enabled=true`}
</CodeBlock>

Register with the control plane:

```bash
CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-data-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: DataPlane
metadata:
  name: default
  namespace: default
spec:
  agent:
    enabled: true
    clientCA:
      value: |
$(echo "$CA_CERT" | sed 's/^/        /')
  gateway:
    organizationVirtualHost: "openchoreoapis.internal"
    publicVirtualHost: "openchoreoapis.localhost"
  secretStoreRef:
    name: default
EOF
```

Verify:

```bash
kubectl get dataplane -n default
kubectl logs -n openchoreo-data-plane -l app=cluster-agent --tail=10
```

---

## Step 3: Setup Build Plane (Optional)

The Build Plane enables OpenChoreo's built-in CI capabilities.

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-build-plane ${versions.helmSource}/openchoreo-build-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-build-plane \\
    --create-namespace \\
    --set external-secrets.enabled=false \\
    --set clusterAgent.enabled=true`}
</CodeBlock>

Register with the control plane:

```bash
BP_CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-build-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: BuildPlane
metadata:
  name: default
  namespace: default
spec:
  agent:
    enabled: true
    clientCA:
      value: |
$(echo "$BP_CA_CERT" | sed 's/^/        /')
EOF
```

Verify:

```bash
kubectl get buildplane -n default
kubectl get pods -n openchoreo-build-plane
```

---

## Step 4: Setup Observability Plane (Optional)

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-observability-plane ${versions.helmSource}/openchoreo-observability-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-observability-plane \\
    --create-namespace \\
    --set openSearch.enabled=true \\
    --set openSearchCluster.enabled=false \\
    --set external-secrets.enabled=false \\
    --set clusterAgent.enabled=true \\
    --timeout 10m`}
</CodeBlock>

Register with the control plane:

```bash
OP_CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-observability-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: ObservabilityPlane
metadata:
  name: default
  namespace: default
spec:
  agent:
    enabled: true
    clientCA:
      value: |
$(echo "$OP_CA_CERT" | sed 's/^/        /')
  observerURL: http://observer.openchoreo-observability-plane.svc.cluster.local:8080
EOF
```

Link the Data Plane (and Build Plane if installed) to use observability:

```bash
kubectl patch dataplane default -n default --type merge -p '{"spec":{"observabilityPlaneRef":"default"}}'
kubectl patch buildplane default -n default --type merge -p '{"spec":{"observabilityPlaneRef":"default"}}'
```

Verify:

```bash
kubectl get observabilityplane -n default
kubectl logs -n openchoreo-observability-plane -l app=cluster-agent --tail=10
```

---

## Access OpenChoreo

<Tabs groupId="cluster-type">
<TabItem value="existing" label="General" default>

| Service | URL |
|---------|-----|
| Console | `http://openchoreo.localhost:8080` |
| API | `http://api.openchoreo.localhost:8080` |
| Deployed Apps | `http://<env>.openchoreoapis.localhost:19080/<component>/...` |

**Default credentials:** `admin@openchoreo.dev` / `Admin@123`

</TabItem>
<TabItem value="kind" label="kind">

| Service | URL |
|---------|-----|
| Console | `http://openchoreo.localhost:8080` |
| API | `http://api.openchoreo.localhost:8080` |

**Default credentials:** `admin@openchoreo.dev` / `Admin@123`

**Accessing Deployed Apps**

With cloud-provider-kind, the data plane gateway gets an external IP. Find the gateway IP and port:

```bash
kubectl get svc gateway-default -n openchoreo-data-plane
```

Access your apps using the `EXTERNAL-IP` and port shown (typically port 19080):
```bash
curl http://<EXTERNAL-IP>:19080/<component>/...
```

For `.localhost` domains to resolve to this IP, add an entry to `/etc/hosts`:
```bash
echo "<EXTERNAL-IP> development.openchoreoapis.localhost" | sudo tee -a /etc/hosts
```

Then you can access apps via:
```
http://<env>.openchoreoapis.localhost:19080/<component>/...
```

</TabItem>
</Tabs>

:::tip Remote Cluster Access
If your cluster is running on a remote VM or server, use SSH tunneling to access OpenChoreo from your local machine:

```bash
ssh -L 8080:localhost:8080 \
      -L 8443:localhost:8443 \
      -L 19080:localhost:19080 \
      -L 19443:localhost:19443 \
      user@remote-host
```

This forwards:
- **8080/8443**: Control Plane UI (Console and API)
- **19080/19443**: Data Plane Gateway (Deployed applications)

Keep this SSH session open and access OpenChoreo via `http://openchoreo.localhost:8080` in your local browser.
:::

---

## Next Steps

1. [Deploy your first component](../deploy-first-component.mdx)
2. Explore the <Link to={`https://github.com/openchoreo/openchoreo/tree/${versions.githubRef}/samples`}>sample applications</Link>

---

## Cleanup

Uninstall OpenChoreo components:

```bash
helm uninstall openchoreo-observability-plane -n openchoreo-observability-plane 2>/dev/null
helm uninstall openchoreo-build-plane -n openchoreo-build-plane 2>/dev/null
helm uninstall openchoreo-data-plane -n openchoreo-data-plane
helm uninstall openchoreo-control-plane -n openchoreo-control-plane
helm uninstall cert-manager -n cert-manager
```

Delete namespaces and plane registrations:

```bash
kubectl delete dataplane default -n default 2>/dev/null
kubectl delete buildplane default -n default 2>/dev/null
kubectl delete observabilityplane default -n default 2>/dev/null
kubectl delete namespace openchoreo-control-plane openchoreo-data-plane openchoreo-build-plane openchoreo-observability-plane cert-manager 2>/dev/null
```

If you created a cluster for this guide:

```bash
# k3d
k3d cluster delete openchoreo

# kind
docker stop cloud-provider-kind
kind delete cluster --name openchoreo

# minikube
minikube delete --profile=openchoreo

# k3s
/usr/local/bin/k3s-uninstall.sh
```

---

## Troubleshooting

### Pods stuck in Pending

```bash
kubectl describe pod <pod-name> -n <namespace>
```

Common causes:
- Insufficient resources (increase RAM/CPU allocation)
- PVC issues (check storage provisioner)

### Agent not connecting

```bash
kubectl logs -n openchoreo-data-plane -l app=cluster-agent --tail=20
kubectl logs -n openchoreo-control-plane -l app=cluster-gateway --tail=20
```

Common issues:
- DataPlane/BuildPlane CR not created
- CA certificate mismatch
- Network connectivity between namespaces

### Gateway pods crash on macOS

If you see "Failed to create temporary file" errors:

```bash
helm upgrade openchoreo-data-plane ... --set gateway.envoy.mountTmpVolume=true
```
