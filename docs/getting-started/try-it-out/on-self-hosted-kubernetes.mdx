---
title: On Self-Hosted Kubernetes
description: Run OpenChoreo on any self-hosted Kubernetes cluster - local machine, VM, or on-premise. Zero cloud costs.
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import Link from '@docusaurus/Link';
import {versions} from '../../_constants.mdx';

# On Self-Hosted Kubernetes

Try OpenChoreo on any self-hosted Kubernetes cluster - whether it's running on your laptop, a VM, or on-premise environment. This is the fastest way to explore OpenChoreo without cloud provider costs.

**What you'll get:**
- Full OpenChoreo installation on your cluster
- All four planes: Control, Data, Build, and Observability
- Access via `.localhost` domains
- ~15-20 minutes to complete

## Prerequisites

<Tabs groupId="cluster-type">
<TabItem value="k3d" label="k3d" default>

[k3d](https://k3d.io) runs k3s in Docker containers.

- **Docker** v26.0+ with at least **8 GB RAM** and **4 CPU** cores allocated
- **Disk space**: ~10 GB free
- **[k3d](https://k3d.io/stable/#installation)** v5.8+
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+

```bash
docker --version && docker info > /dev/null
k3d --version
kubectl version --client
helm version --short
```

:::note Colima Users
Set `K3D_FIX_DNS=0` when creating clusters to avoid DNS issues. See [k3d-io/k3d#1449](https://github.com/k3d-io/k3d/issues/1449).
:::

**Create Cluster**

<CodeBlock language="bash">
{`curl -s https://raw.githubusercontent.com/openchoreo/openchoreo/${versions.githubRef}/install/k3d/single-cluster/config.yaml | k3d cluster create --config=-`}
</CodeBlock>

This creates a cluster named `openchoreo` with:
- 1 server node (no agents)
- Port mappings: Control Plane (8080/8443), Data Plane (19080/19443), Build Plane (10081)
- kubectl context set to `k3d-openchoreo`

**Install cert-manager**

```bash
helm upgrade --install cert-manager oci://quay.io/jetstack/charts/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --set crds.enabled=true
```

Wait for cert-manager to be ready:

```bash
kubectl wait --for=condition=available deployment/cert-manager -n cert-manager --timeout=120s
```

</TabItem>
<TabItem value="existing" label="Existing Cluster">

- **Kubernetes 1.32+** cluster with at least **8 GB RAM** and **4 CPU** cores
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+ configured to access your cluster
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+
- **cert-manager** installed in your cluster

:::note Rancher Desktop Users
Use **containerd** as the container runtime. If you plan to use the Build Plane, configure your runtime to allow HTTP registries before proceedingâ€”see the note in [Step 3: Setup Build Plane](#step-3-setup-build-plane-optional).
:::

```bash
kubectl version
helm version --short
kubectl get nodes
kubectl auth can-i '*' '*' --all-namespaces
```

**Note Your Ingress Configuration**

Check if you have an ingress controller:

```bash
kubectl get ingressclass
```

Common configurations:
- **Rancher Desktop**: Built-in Traefik on ports 80/443, class name `traefik`
- **Docker Desktop**: No default ingress
- **OrbStack**: Built-in ingress on ports 80/443

</TabItem>
</Tabs>

---

## Step 1: Setup Control Plane

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-control-plane ${versions.helmSource}/openchoreo-control-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-control-plane \\
    --create-namespace \\
    --set global.baseDomain=openchoreo.localhost \\
    --set global.port=":8080" \\
    --set traefik.ports.web.exposedPort=8080 \\
    --set traefik.ports.websecure.exposedPort=8443 \\
    --set thunder.configuration.server.publicUrl=http://thunder.openchoreo.localhost:8080 \\
    --set thunder.configuration.gateClient.hostname=thunder.openchoreo.localhost \\
    --set thunder.configuration.gateClient.port=8080 \\
    --set thunder.configuration.gateClient.scheme="http"`}
</CodeBlock>

Wait for deployment to be ready:

```bash
kubectl wait -n openchoreo-control-plane --for=condition=available --timeout=300s deployment --all
kubectl wait -n openchoreo-control-plane --for=condition=complete  job --all
```

---

## Step 2: Setup Data Plane

:::note macOS Users
Due to Rosetta emulation issues, macOS users (Rancher Desktop, Docker Desktop, k3d, kind, or Colima) should add `--set gateway.envoy.mountTmpVolume=true`. Non-macOS users can omit this flag if needed.
:::

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-data-plane ${versions.helmSource}/openchoreo-data-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-data-plane \\
    --create-namespace \\
    --set gateway.httpPort=19080 \\
    --set gateway.httpsPort=19443 \\
    --set external-secrets.enabled=true \\
    --set gateway.envoy.mountTmpVolume=true`}
</CodeBlock>

Create a Certificate for Gateway TLS:

```bash
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: openchoreo-gateway-tls
  namespace: openchoreo-data-plane
spec:
  secretName: openchoreo-gateway-tls
  issuerRef:
    name: openchoreo-selfsigned-issuer
    kind: ClusterIssuer
  dnsNames:
    - "*.openchoreoapis.localhost"
EOF
```

Register with the control plane:

:::important PlaneID Consistency
The `planeID` in the DataPlane CR must match the `clusterAgent.planeId` Helm value (default: `"default-dataplane"`). If you customized `clusterAgent.planeId` during Helm installation, update the `planeID` field below to match.
:::

```bash
CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-data-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: DataPlane
metadata:
  name: default
  namespace: default
spec:
  planeID: "default-dataplane"
  clusterAgent:
    clientCA:
      value: |
$(echo "$CA_CERT" | sed 's/^/        /')
  gateway:
    organizationVirtualHost: "openchoreoapis.internal"
    publicVirtualHost: "openchoreoapis.localhost"
  secretStoreRef:
    name: default
EOF
```

Verify:

```bash
kubectl get dataplane -n default
kubectl logs -n openchoreo-data-plane -l app=cluster-agent --tail=10
```

---

## Step 3: Setup Build Plane (Optional)

The Build Plane enables OpenChoreo's built-in CI capabilities.



<Tabs groupId="cluster-type">
<TabItem value="k3d" label="k3d" default>

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-build-plane ${versions.helmSource}/openchoreo-build-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-build-plane \\
    --create-namespace \\
    --set external-secrets.enabled=false \\
    --set global.defaultResources.registry.endpoint=host.k3d.internal:10082 \\
    --set registry.service.type=LoadBalancer`}
</CodeBlock>

</TabItem>
<TabItem value="existing" label="Existing Cluster">

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-build-plane ${versions.helmSource}/openchoreo-build-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-build-plane \\
    --create-namespace \\
    --set external-secrets.enabled=false \\
    --set global.defaultResources.registry.endpoint=<REGISTRY_ENDPOINT> \\
    --set registry.service.type=LoadBalancer \\
    --set registry.service.port=10082`}
</CodeBlock>

:::note Registry Configuration

The Build Plane deploys an HTTP container registry. You may need to configure two things based on your cluster setup:

**1. Registry Endpoint**: Update `global.defaultResources.registry.endpoint` to an address accessible from both the build pods (for pushing) and the kubelet (for pulling). Common values:
- `host.docker.internal:10082` - For clusters that support host.docker.internal
- `<node-ip>:10082` - Using the node's IP address
- `registry.openchoreo-build-plane.svc.cluster.local:5000` - In-cluster only (won't work if kubelet can't reach cluster services)

**2. HTTP Registry Access**: Container runtimes default to HTTPS for registries. If image pulls fail with "http: server gave HTTP response to HTTPS client", configure your container runtime to allow HTTP for this registry. For Rancher Desktop users, see [Configuring Private Registries](https://docs.rancherdesktop.io/how-to-guides/mirror-private-registry/). For other platforms, consult your Kubernetes distribution's documentation on configuring insecure registries.

:::

</TabItem>
</Tabs>

Register with the control plane:

:::important PlaneID Consistency
The `planeID` in the BuildPlane CR must match the `clusterAgent.planeId` Helm value (default: `"default-buildplane"`). If you customized `clusterAgent.planeId` during Helm installation, update the `planeID` field below to match.
:::

```bash
BP_CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-build-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: BuildPlane
metadata:
  name: default
  namespace: default
spec:
  planeID: "default-buildplane"
  clusterAgent:
    clientCA:
      value: |
$(echo "$BP_CA_CERT" | sed 's/^/        /')
EOF
```

Verify:

```bash
kubectl get buildplane -n default
kubectl logs -n openchoreo-build-plane -l app=cluster-agent --tail=10
```

---

## Step 4: Setup Observability Plane (Optional)

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-observability-plane ${versions.helmSource}/openchoreo-observability-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-observability-plane \\
    --create-namespace \\
    --set openSearch.enabled=true \\
    --set openSearchCluster.enabled=false \\
    --set security.oidc.jwksUrl="http://thunder-service.openchoreo-control-plane.svc.cluster.local:8090/oauth2/jwks" \\
    --set external-secrets.enabled=false \\
    --timeout 10m`}
</CodeBlock>

Register with the control plane:

:::important PlaneID Consistency
The `planeID` in the ObservabilityPlane CR must match the `clusterAgent.planeId` Helm value (default: `"default-observabilityplane"`). If you customized `clusterAgent.planeId` during Helm installation, update the `planeID` field below to match.
:::

```bash
OP_CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-observability-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: ObservabilityPlane
metadata:
  name: default
  namespace: default
spec:
  planeID: "default-observabilityplane"
  clusterAgent:
    clientCA:
      value: |
$(echo "$OP_CA_CERT" | sed 's/^/        /')
  observerURL: http://observer.openchoreo-observability-plane.svc.cluster.local:8080
EOF
```

Link the Data Plane (and Build Plane if installed) to use observability:

```bash
kubectl patch dataplane default -n default --type merge -p '{"spec":{"observabilityPlaneRef":"default"}}'
kubectl patch buildplane default -n default --type merge -p '{"spec":{"observabilityPlaneRef":"default"}}'
```

Verify:

```bash
kubectl get observabilityplane -n default
kubectl logs -n openchoreo-observability-plane -l app=cluster-agent --tail=10
```

---

## Access OpenChoreo

| Service | URL |
|---------|-----|
| Console | `http://openchoreo.localhost:8080` |
| API | `http://api.openchoreo.localhost:8080` |
| Deployed Apps | `http://<env>.openchoreoapis.localhost:19080/<component>/...` |

**Default credentials:** `admin@openchoreo.dev` / `Admin@123`

:::tip Remote Cluster Access
If your cluster is running on a remote VM or server, use SSH tunneling to access OpenChoreo from your local machine:

```bash
ssh -L 8080:localhost:8080 \
      -L 8443:localhost:8443 \
      -L 19080:localhost:19080 \
      -L 19443:localhost:19443 \
      user@remote-host
```

This forwards:
- **8080/8443**: Control Plane UI (Console and API)
- **19080/19443**: Data Plane Gateway (Deployed applications)

Keep this SSH session open and access OpenChoreo via `http://openchoreo.localhost:8080` in your local browser.
:::

---

## Next Steps

1. [Deploy your first component](../deploy-first-component.mdx)
2. Explore the <Link to={`https://github.com/openchoreo/openchoreo/tree/${versions.githubRef}/samples`}>sample applications</Link>

---

## Cleanup

Uninstall OpenChoreo components:

```bash
helm uninstall openchoreo-observability-plane -n openchoreo-observability-plane 2>/dev/null
helm uninstall openchoreo-build-plane -n openchoreo-build-plane 2>/dev/null
helm uninstall openchoreo-data-plane -n openchoreo-data-plane
helm uninstall openchoreo-control-plane -n openchoreo-control-plane
helm uninstall cert-manager -n cert-manager
```

Delete namespaces and plane registrations:

```bash
kubectl delete dataplane default -n default 2>/dev/null
kubectl delete buildplane default -n default 2>/dev/null
kubectl delete observabilityplane default -n default 2>/dev/null
kubectl delete namespace openchoreo-control-plane openchoreo-data-plane openchoreo-build-plane openchoreo-observability-plane cert-manager 2>/dev/null
```

If you created a k3d cluster for this guide:

```bash
k3d cluster delete openchoreo
```

---

## Troubleshooting

### Pods stuck in Pending

```bash
kubectl describe pod <pod-name> -n <namespace>
```

Common causes:
- Insufficient resources (increase RAM/CPU allocation)
- PVC issues (check storage provisioner)

### Agent not connecting

```bash
kubectl logs -n openchoreo-data-plane -l app=cluster-agent --tail=20
kubectl logs -n openchoreo-control-plane -l app=cluster-gateway --tail=20
```

Common issues:
- DataPlane/BuildPlane CR not created
- **PlaneID mismatch**: The `planeID` in the plane CR must match the `clusterAgent.planeId` Helm value
- CA certificate mismatch
- Network connectivity between namespaces

### Gateway pods crash on macOS

If you see "Failed to create temporary file" errors:

```bash
helm upgrade openchoreo-data-plane ... --set gateway.envoy.mountTmpVolume=true
```

