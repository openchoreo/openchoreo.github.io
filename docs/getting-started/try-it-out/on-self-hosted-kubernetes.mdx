---
title: On Self-Hosted Kubernetes
description: Run OpenChoreo on any self-hosted Kubernetes cluster - local machine, VM, or on-premise. Zero cloud costs.
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import Link from '@docusaurus/Link';
import {versions} from '../../_constants.mdx';

# On Self-Hosted Kubernetes

Try OpenChoreo on any self-hosted Kubernetes cluster, whether it's running on your laptop, a VM, or on-premise environment. This is the fastest way to explore OpenChoreo without cloud provider costs.

**What you'll get:**
- Full OpenChoreo installation on your cluster
- All four planes: Control, Data, Build, and Observability
- Access via `.localhost` domains
- ~15-20 minutes to complete

## Prerequisites

<Tabs groupId="cluster-type">
<TabItem value="k3d" label="k3d" default>

[k3d](https://k3d.io) runs k3s in Docker containers.

- **Docker** v26.0+ with at least **8 GB RAM** and **4 CPU** cores allocated
- **Disk space**: ~10 GB free
- **[k3d](https://k3d.io/stable/#installation)** v5.8+
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+

```bash
docker --version && docker info > /dev/null
k3d --version
kubectl version --client
helm version --short
```

:::tip Mac Users
For optimal compatibility and to avoid buildpack build issues, we recommend using [Colima](https://github.com/abiosoft/colima) with VZ and Rosetta support:

```bash
colima start --vm-type=vz --vz-rosetta --cpu 4 --memory 8
```
:::

:::note Colima Users
Set `K3D_FIX_DNS=0` when creating clusters to avoid DNS issues. See [k3d-io/k3d#1449](https://github.com/k3d-io/k3d/issues/1449).
:::

**Create Cluster**

<CodeBlock language="bash">
{`curl -s https://raw.githubusercontent.com/openchoreo/openchoreo/${versions.githubRef}/install/k3d/single-cluster/config.yaml | k3d cluster create --config=-`}
</CodeBlock>

This creates a cluster named `openchoreo` with port mappings for Control Plane (8080/8443), Data Plane (19080/19443), and Build Plane (10081). Your kubectl context is automatically set to `k3d-openchoreo`.

:::note Generate machine id (Required if Observability will be enabled)

Fluent Bit (used for observability log collection) requires a unique Machine ID (/etc/machine-id) to start. By default, k3d node containers do not generate this file.
If you are enabling observability, you must manually generate this ID on every node in your cluster before installation:

```bash
docker exec <node name> sh -c "cat /proc/sys/kernel/random/uuid | tr -d '-' > /etc/machine-id"
```

For example, to generate the machine ID for the k3d-openchoreo-op-server-0 node:

```bash
docker exec k3d-openchoreo-op-server-0 sh -c "cat /proc/sys/kernel/random/uuid | tr -d '-' > /etc/machine-id"
```

:::

**Install cert-manager**

```bash
helm upgrade --install cert-manager oci://quay.io/jetstack/charts/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --set crds.enabled=true
```

Wait for cert-manager to be ready:

```bash
kubectl wait --for=condition=available deployment/cert-manager -n cert-manager --timeout=120s
```

</TabItem>
<TabItem value="existing" label="Existing Cluster">

- **Kubernetes 1.32+** cluster with at least **8 GB RAM** and **4 CPU** cores
- **[kubectl](https://kubernetes.io/docs/tasks/tools/)** v1.32+ configured to access your cluster
- **[Helm](https://helm.sh/docs/intro/install/)** v3.12+
- **cert-manager** installed in your cluster

:::note Rancher Desktop Users
Use **containerd** as the container runtime. If you plan to use the Build Plane, configure your runtime to allow HTTP registries before proceeding—see the note in [Step 3: Setup Build Plane](#step-3-setup-build-plane-optional).
:::

```bash
kubectl version
helm version --short
kubectl get nodes
kubectl auth can-i '*' '*' --all-namespaces
```

**Note Your Ingress Configuration**

Check if you have an ingress controller:

```bash
kubectl get ingressclass
```

Common configurations:
- **Rancher Desktop**: Built-in Traefik on ports 80/443, class name `traefik`
- **Docker Desktop**: No default ingress
- **OrbStack**: Built-in ingress on ports 80/443

:::warning Free Up Ports 80/443
OpenChoreo's control plane gateway needs to bind to ports 80/443. If you have Traefik or another ingress controller running on these ports, you must either:

- Disable it (e.g., for Rancher Desktop: disable Traefik in Preferences → Kubernetes)
- Configure it to use different ports

You can verify port availability with:

```bash
# Check if anything is listening on port 80
lsof -i :80
```

:::

</TabItem>
</Tabs>

---

## Step 1: Setup Control Plane

:::note macOS Users
Due to Rosetta emulation issues, macOS users (Rancher Desktop, Docker Desktop, k3d, kind, or Colima) should add `--set gateway.envoy.mountTmpVolume=true`. Non-macOS users can omit this flag if needed.
:::
<Tabs groupId="cluster-type">
<TabItem value="k3d" label="k3d" default>

<CodeBlock language="bash">
  {`helm upgrade --install openchoreo-control-plane ${versions.helmSource}/openchoreo-control-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-control-plane \\
    --create-namespace \\
    --set global.baseDomain=openchoreo.localhost \\
    --set global.port=":8080" \\
    --set gateway.httpPort=80 \\
    --set gateway.httpsPort=443 \\
    --set thunder.configuration.server.publicUrl=http://thunder.openchoreo.localhost:8080 \\
    --set thunder.configuration.gateClient.hostname=thunder.openchoreo.localhost \\
    --set thunder.configuration.gateClient.port=8080 \\
    --set thunder.configuration.gateClient.scheme="http" \\
    --set gateway.envoy.mountTmpVolume=true`}
</CodeBlock>

</TabItem>
<TabItem value="existing" label="Existing Cluster">

<CodeBlock language="bash">
  {`helm upgrade --install openchoreo-control-plane ${versions.helmSource}/openchoreo-control-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-control-plane \\
    --create-namespace \\
    --set global.baseDomain=openchoreo.localhost \\
     --set global.port=":80" \\
    --set gateway.httpPort=80 \\
    --set gateway.httpsPort=443 \\
    --set thunder.configuration.server.publicUrl=http://thunder.openchoreo.localhost:80 \\
    --set thunder.configuration.gateClient.hostname=thunder.openchoreo.localhost \\
    --set thunder.configuration.gateClient.port=80 \\
    --set thunder.configuration.gateClient.scheme="http" \\
    --set gateway.envoy.mountTmpVolume=true`}
</CodeBlock>

</TabItem>
</Tabs>

This installs the control plane into the `openchoreo-control-plane` namespace, with these settings:

- `global.baseDomain`: the base domain for all services. The console will be at `openchoreo.localhost`, the API at `api.openchoreo.localhost`.
- `global.port`: appended to URLs since we're using non-standard port 8080.
- `gateway.httpPort` and `gateway.httpsPort`: the ports where KGateway listens for incoming traffic.
- `thunder.configuration.*`: configures Thunder, the built-in identity provider. These settings tell Thunder where it's accessible and how to reach the API gateway.

This installs:

- `controller-manager`: the controllers that reconcile OpenChoreo resources and manage the platform lifecycle.
- `openchoreo-api`: REST API server that the console and CLI talk to.
- `backstage`: the web console for managing your platform.
- `thunder`: built-in identity provider handling authentication and OAuth flows.
- `cluster-gateway`: accepts WebSocket connections from cluster-agents in remote planes.
- `kgateway`: gateway controller for routing external traffic to services.
- OpenChoreo CRDs: Organization, Project, Component, Environment, DataPlane, BuildPlane, and others that define the platform's API.

The control plane is OpenChoreo's brain. In production, you'd typically run this in its own dedicated cluster, isolated from your workloads.

For all available configuration options, see the [Control Plane Helm Reference](../../reference/helm/control-plane.mdx).

```bash
kubectl wait -n openchoreo-control-plane --for=condition=available --timeout=300s deployment --all
kubectl wait -n openchoreo-control-plane --for=condition=complete job --all
```

Create a Certificate for Gateway TLS:

```bash
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: control-plane-tls
  namespace: openchoreo-control-plane
spec:
  secretName: control-plane-tls
  issuerRef:
    name: openchoreo-selfsigned-issuer
    kind: ClusterIssuer
  dnsNames:
    - "*.openchoreo.localhost"
EOF
```

---

## Step 2: Setup Data Plane

<CodeBlock language="bash">
  {`helm upgrade --install openchoreo-data-plane ${versions.helmSource}/openchoreo-data-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-data-plane \\
    --create-namespace \\
    --set gateway.httpPort=19080 \\
    --set gateway.httpsPort=19443 \\
    --set external-secrets.enabled=true \\
    --set gatewayController.enabled=false \\
    --set gateway.envoy.mountTmpVolume=true \\
    --set gateway.selfSignedIssuer.enabled=false`}
</CodeBlock>

This installs the data plane into the `openchoreo-data-plane` namespace, with these settings:

- `gateway.httpPort` and `gateway.httpsPort`: the ports where KGateway listens for traffic to your applications. We use 19080/19443 to keep it distinct from the control plane's ports.
- `external-secrets.enabled`: installs the External Secrets Operator for syncing secrets from external stores.
- `gateway.envoy.mountTmpVolume`: fixes Envoy crashes on macOS. Non-macOS users can omit this flag.

This installs:

- `cluster-agent`: maintains a WebSocket connection to the control plane's cluster-gateway. This is how the control plane sends deployment instructions to the data plane.
- `gateway`: KGateway with Envoy proxy that routes incoming traffic to your deployed applications.
- `fluent-bit`: collects logs from your workloads and forwards them to the observability plane.
- `external-secrets`: syncs secrets from external secret stores like Vault or AWS Secrets Manager.
- Gateway API CRDs: Gateway, HTTPRoute, and other resources for traffic routing.

The data plane is where your workloads actually run. In this guide we're installing it in the same cluster as the control plane, but in production you'd typically have it in a completely separate cluster. This separation is intentional: your application code never runs alongside the control plane, and the control plane's credentials are never exposed to your workloads.

For all available configuration options, see the [Data Plane Helm Reference](../../reference/helm/data-plane.mdx).

**Create a Certificate for Gateway TLS**

```bash
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: openchoreo-gateway-tls
  namespace: openchoreo-data-plane
spec:
  secretName: openchoreo-gateway-tls
  issuerRef:
    name: openchoreo-selfsigned-issuer
    kind: ClusterIssuer
  dnsNames:
    - "*.openchoreoapis.localhost"
EOF
```

**Register with the Control Plane**

```bash
CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-data-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)
```

The control plane only accepts agent connections signed by a CA it recognizes. When you installed the data plane, cert-manager generated a CA and used it to sign the cluster-agent's client certificate. This command extracts that CA so you can tell the control plane about it.

```bash
kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: DataPlane
metadata:
  name: default
  namespace: default
spec:
  planeID: "default-dataplane"
  clusterAgent:
    clientCA:
      value: |
$(echo "$CA_CERT" | sed 's/^/        /')
  gateway:
    organizationVirtualHost: "openchoreoapis.internal"
    publicVirtualHost: "openchoreoapis.localhost"
  secretStoreRef:
    name: default
EOF
```

This creates a DataPlane resource that tells the control plane about your data plane, with these settings:

- `planeID`: identifies this data plane. Must match what the cluster-agent was configured with, which defaults to `default-dataplane`.
- `clusterAgent.clientCA`: the CA certificate that signed the agent's client certificate. The control plane uses this to verify incoming connections.
- `gateway.publicVirtualHost`: where your deployed applications become accessible. When you deploy a component later, it'll be reachable at something like `http://dev.openchoreoapis.localhost:19080/your-component/`.
- `secretStoreRef`: references the External Secrets ClusterSecretStore for managing secrets.

**Verify**

```bash
kubectl get dataplane -n default
```

The cluster-agent should now be connected. You can check its logs:

```bash
kubectl logs -n openchoreo-data-plane -l app=cluster-agent --tail=10
```

---

## Step 3: Setup Build Plane (Optional)

The Build Plane runs Argo Workflows to build container images from your source code. You need a container registry to store built images.

<Tabs groupId="cluster-type">
<TabItem value="k3d" label="k3d" default>

Install a container registry in your cluster:

```bash
helm repo add twuni https://helm.twun.io
helm repo update

helm install registry twuni/docker-registry \
  --namespace openchoreo-build-plane \
  --create-namespace \
  --set persistence.enabled=true \
  --set persistence.size=10Gi \
  --set service.type=ClusterIP
```

Wait for the registry to be ready:

```bash
kubectl wait --for=condition=available deployment/registry-docker-registry -n openchoreo-build-plane --timeout=120s
```

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-build-plane ${versions.helmSource}/openchoreo-build-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-build-plane \\
    --set external-secrets.enabled=false \\
    --set global.defaultResources.registry.host=registry-docker-registry.openchoreo-build-plane.svc.cluster.local:5000 \\
    --set global.defaultResources.registry.tlsVerify=false`}
</CodeBlock>

</TabItem>
<TabItem value="existing" label="Existing Cluster">

Choose a registry option:

<Tabs groupId="registry-type">
<TabItem value="ttl-sh" label="ttl.sh (Easiest)" default>

[ttl.sh](https://ttl.sh) is a free, anonymous registry. Images expire after 24 hours. No setup required.

```bash
export REGISTRY_PREFIX=$(cat /dev/urandom | LC_ALL=C tr -dc 'a-z0-9' | fold -w 8 | head -n 1)
echo "Your registry prefix: $REGISTRY_PREFIX"
```

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-build-plane ${versions.helmSource}/openchoreo-build-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-build-plane \\
    --create-namespace \\
    --set external-secrets.enabled=false \\
    --set global.defaultResources.registry.host=ttl.sh \\
    --set global.defaultResources.registry.repoPath=$REGISTRY_PREFIX \\
    --set global.defaultResources.registry.tlsVerify=true`}
</CodeBlock>

:::note
Images on ttl.sh expire after 24 hours. For persistent storage, use the host registry option or a cloud provider registry.
:::

</TabItem>
<TabItem value="host-registry" label="Host Registry">

Run a registry on your host machine using Docker. This works well when your cluster can reach the host via `host.docker.internal`.

```bash
docker run -d --name registry -p 5050:5000 registry:3
```

:::note HTTP Registry Access
Configure your container runtime to allow this insecure registry. For Rancher Desktop, see [Configuring Private Registries](https://docs.rancherdesktop.io/how-to-guides/mirror-private-registry/).
:::

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-build-plane ${versions.helmSource}/openchoreo-build-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-build-plane \\
    --create-namespace \\
    --set external-secrets.enabled=false \\
    --set global.defaultResources.registry.host=host.docker.internal:5050 \\
    --set global.defaultResources.registry.tlsVerify=false`}
</CodeBlock>

:::tip
If `host.docker.internal` doesn't resolve in your cluster, use your machine's IP address instead.
:::

</TabItem>
</Tabs>

</TabItem>
</Tabs>

This installs:

- `cluster-agent`: connects to the control plane to receive build instructions.
- `argo-workflows`: executes the actual build pipelines as Kubernetes workflows.
- Argo Workflows CRDs: Workflow, WorkflowTemplate, and other resources for defining build pipelines.

For all available configuration options, see the [Build Plane Helm Reference](../../reference/helm/build-plane.mdx). For production deployments, see [Container Registry Configuration](../../operations/container-registry-configuration.mdx).

**Register with the Control Plane**

```bash
BP_CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-build-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: BuildPlane
metadata:
  name: default
  namespace: default
spec:
  planeID: "default-buildplane"
  clusterAgent:
    clientCA:
      value: |
$(echo "$BP_CA_CERT" | sed 's/^/        /')
EOF
```

The `planeID` must match the helm chart's default of `default-buildplane`. Like the data plane, the build plane could run in a completely separate cluster if you wanted to isolate your CI workloads.

**Verify**

```bash
kubectl get buildplane -n default
kubectl logs -n openchoreo-build-plane -l app=cluster-agent --tail=10
```

---

## Step 4: Setup Observability Plane (Optional)

<CodeBlock language="bash">
{`helm upgrade --install openchoreo-observability-plane ${versions.helmSource}/openchoreo-observability-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-observability-plane \\
    --create-namespace \\
    --set openSearch.enabled=true \\
    --set openSearchCluster.enabled=false \\
    --set security.oidc.jwksUrl="http://thunder-service.openchoreo-control-plane.svc.cluster.local:8090/oauth2/jwks" \\
    --set security.oidc.tokenUrl="http://thunder-service.openchoreo-control-plane.svc.cluster.local:8090/oauth2/token" \\
    --set external-secrets.enabled=false \\
    --timeout 10m`}
</CodeBlock>

This installs the observability plane with these settings:

- `openSearch.enabled`: deploys OpenSearch for storing logs and traces.
- `openSearchCluster.enabled`: set to false to use the simpler single-node deployment instead of the operator-based cluster.
- `security.oidc.jwksUrl`: the JWKS endpoint for validating JWT tokens. This points to Thunder's JWKS endpoint so the Observer API can authenticate requests.

This installs:

- `cluster-agent`: connects to the control plane.
- `opensearch`: stores logs and traces from your workloads.
- `observer`: REST API that abstracts OpenSearch. The console and other components query logs through this instead of talking to OpenSearch directly.
- `opentelemetry-collector`: receives traces and metrics from your applications.
- `prometheus`: collects metrics from your workloads (via kube-prometheus-stack).

The observability plane collects logs, metrics, and traces from your data and build planes. Like the other planes, it could run in a completely separate cluster in production.

For all available configuration options, see the [Observability Plane Helm Reference](../../reference/helm/observability-plane.mdx).

**Register with the Control Plane**

```bash
OP_CA_CERT=$(kubectl get secret cluster-agent-tls -n openchoreo-observability-plane -o jsonpath='{.data.ca\.crt}' | base64 -d)

kubectl apply -f - <<EOF
apiVersion: openchoreo.dev/v1alpha1
kind: ObservabilityPlane
metadata:
  name: default
  namespace: default
spec:
  planeID: "default-observabilityplane"
  clusterAgent:
    clientCA:
      value: |
$(echo "$OP_CA_CERT" | sed 's/^/        /')
  observerURL: http://observer.openchoreo-observability-plane.svc.cluster.local:8080
EOF
```

The `observerURL` tells the control plane where to find the Observer API.

**Link Other Planes to Observability**

```bash
kubectl patch dataplane default -n default --type merge -p '{"spec":{"observabilityPlaneRef":"default"}}'
kubectl patch buildplane default -n default --type merge -p '{"spec":{"observabilityPlaneRef":"default"}}'
```

This tells the data plane and build plane to send their logs and traces to this observability plane.

**Verify**

```bash
kubectl get observabilityplane -n default
kubectl logs -n openchoreo-observability-plane -l app=cluster-agent --tail=10
```

Enable logs collection:
<CodeBlock language="bash">
{`helm upgrade --install openchoreo-observability-plane ${versions.helmSource}/openchoreo-observability-plane \\
    --version ${versions.helmChart} \\
    --namespace openchoreo-observability-plane \\
    --reuse-values \\
    --set fluent-bit.enabled=true \\
    --timeout 10m`}
</CodeBlock>

---

## Access OpenChoreo

| Service | URL |
|---------|-----|
| Console | `http://openchoreo.localhost:8080` |
| API | `http://api.openchoreo.localhost:8080` |
| Deployed Apps | `http://<env>.openchoreoapis.localhost:19080/<component>/...` |

**Default credentials:** `admin@openchoreo.dev` / `Admin@123`

:::tip Remote Cluster Access
If your cluster is running on a remote VM or server, use SSH tunneling to access OpenChoreo from your local machine:

```bash
ssh -L 8080:localhost:8080 \
      -L 8443:localhost:8443 \
      -L 19080:localhost:19080 \
      -L 19443:localhost:19443 \
      user@remote-host
```

This forwards the Control Plane UI (8080/8443) and Data Plane Gateway (19080/19443) to your local machine. Keep this SSH session open and access OpenChoreo via `http://openchoreo.localhost:8080` in your local browser.
:::

---

## Moving to Production

This guide provides a quick way to explore OpenChoreo. For production deployments, follow these guides to harden your setup:

1. **Identity & Security**: Replace default credentials with a real Identity Provider.
   - [Identity Configuration](../../operations/identity-configuration.mdx) (Google, Okta, etc.)
   - [Secret Management](../../operations/secret-management.mdx) (Vault, AWS Secrets Manager)

2. **Networking & Domains**: Move away from localhost to your own domains.
   - [Deployment Topology](../../operations/deployment-topology.mdx) (TLS certificates, Multi-region, Multi-cluster)

3. **Infrastructure**: Scale out and isolate your planes.
   - [Multi-Cluster Connectivity](../../operations/multi-cluster-connectivity.mdx) (Isolate Control Plane from Data Planes)
   - [Observability](../../operations/observability-alerting.mdx) (Configure persistent OpenSearch and retention)

## Next Steps

1. [Deploy your first component](../deploy-first-component.mdx) to see OpenChoreo in action.
2. Explore the <Link to={`https://github.com/openchoreo/openchoreo/tree/${versions.githubRef}/samples`}>sample applications</Link>.

---

## Cleanup

Uninstall OpenChoreo components:

```bash
helm uninstall openchoreo-observability-plane -n openchoreo-observability-plane 2>/dev/null
helm uninstall openchoreo-build-plane -n openchoreo-build-plane 2>/dev/null
helm uninstall openchoreo-data-plane -n openchoreo-data-plane
helm uninstall openchoreo-control-plane -n openchoreo-control-plane
helm uninstall cert-manager -n cert-manager
```

Delete namespaces and plane registrations:

```bash
kubectl delete dataplane default -n default 2>/dev/null
kubectl delete buildplane default -n default 2>/dev/null
kubectl delete observabilityplane default -n default 2>/dev/null
kubectl delete namespace openchoreo-control-plane openchoreo-data-plane openchoreo-build-plane openchoreo-observability-plane cert-manager 2>/dev/null
```

If you created a k3d cluster for this guide:

```bash
k3d cluster delete openchoreo
```

---

## Troubleshooting

### Pods stuck in Pending

```bash
kubectl describe pod <pod-name> -n <namespace>
```

Common causes:
- Insufficient resources (increase RAM/CPU allocation)
- PVC issues (check storage provisioner)

### Agent not connecting

```bash
kubectl logs -n openchoreo-data-plane -l app=cluster-agent --tail=20
kubectl logs -n openchoreo-control-plane -l app=cluster-gateway --tail=20
```

Common issues:
- DataPlane/BuildPlane CR not created
- **PlaneID mismatch**: The `planeID` in the plane CR must match the `clusterAgent.planeId` Helm value
- CA certificate mismatch
- Network connectivity between namespaces

### Gateway pods crash on macOS

If you see "Failed to create temporary file" errors:

```bash
helm upgrade openchoreo-data-plane ... --set gateway.envoy.mountTmpVolume=true
```
